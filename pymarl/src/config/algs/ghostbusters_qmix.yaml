name: ghostbusters_qmix

# --- Agent / Controller ---
agent: rnn                 # built-in registered agent
mac: basic_mac             # multi-agent controller
runner: episode

# RNN agent
use_rnn: True
rnn_hidden_dim: 128        # Increased from 64 for richer representations

# Action selection & outputs
agent_output_type: q
action_selector: epsilon_greedy

# --- Mixer & Learner ---
mixer: qmix                # QMIX for non-linear value combination
learner: q_learner         # learner class for (DQN-style) VDN/QMIX

# QMIX-specific hyperparameters
mixing_embed_dim: 32       # Embedding dimension for mixer hypernetworks
hypernet_layers: 2         # Number of layers in hypernetworks
hypernet_embed: 64         # Hidden size of hypernetworks

# --- Training ---
gamma: 0.99
double_q: True

optimizer: adam
lr: 0.0003                 # Even slower for QMIX stability
critic_lr: 0.0003
grad_norm_clip: 5          # TIGHTER clipping (was 10, gradients were 44k!)

batch_size: 64             # Larger batches for QMIX stability
buffer_size: 50000         # Large buffer for diverse experiences
batch_size_run: 1          # steps per env before insert

t_max: 600000              # Extended training for better convergence

# Epsilon-greedy exploration - EXTENDED FOR SPARSE REWARDS
epsilon_start: 1
epsilon_finish: 0.05       # Lower floor for continued exploration (was 0.1)
epsilon_anneal_time: 400000  # Even slower anneal for QMIX

# Logging with reward normalization for stability
standardise_returns: False
standardise_rewards: True   # Normalize rewards to prevent Q-explosion
reward_standardisation_alpha: 0.01  # Running mean/std update rate

# --- Logging / Eval ---
test_nepisode: 5
test_interval: 10000        # test every 10k env steps
log_interval: 2000
save_model: True

# Exploration during evaluation (tests)
evaluation_epsilon: 0.1     # small randomization at test time

# Misc flags some forks read (safe defaults)
obs_agent_id: False
obs_last_action: False
state_last_action: False
mask_before_softmax: False

# Target network update settings
target_update_interval: 200           # hard update every 200 steps
target_update_interval_or_tau: 200    # duplicate for newer code paths
tau: 0.005                            # only used if the code uses soft updates
