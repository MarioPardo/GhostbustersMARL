{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23c83143",
   "metadata": {},
   "source": [
    "<h1> Project Progress </h1>\n",
    "\n",
    "This file will track the progress of this project, including major steps, code changes, results, etc. \n",
    "A ipynb file is best as it will allow for code, writing, etc and best formatting. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa38aa37",
   "metadata": {},
   "source": [
    "<h1> Baseline: Stage 1 <h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669c5e33",
   "metadata": {},
   "source": [
    "Full Observability\n",
    "Hold time: 3 step\n",
    "No extraction\n",
    "No ghost movement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36fecd90",
   "metadata": {},
   "source": [
    "<h2> VDN </h2>\n",
    "Did not work. This was the last and best attempt\n",
    "\n",
    "[INFO 10:16:01] my_main Recent Stats | t_env:     199894 | Episode:      676\n",
    "ep_length_mean:          300.0000       epsilon:                   0.0500       grad_norm:               2968.1982      hold_mean:                 0.0000\n",
    "loss:                    9660.9391      q_taken_mean:              4.3232       return_mean:             462.8749       return_std:              980.8482\n",
    "success_mean:              0.0000       t_mean:                  300.0000       target_mean:               5.4364       td_error_abs:             10.1837\n",
    "test_ep_length_mean:     286.3500       test_hold_mean:            0.2500       test_return_mean:        841.0543       test_return_std:         1298.6543\n",
    "test_success_mean:         0.0500       test_t_mean:             286.3500\n",
    "[INFO 10:16:01] my_main Finished Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "af7e7d8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"Baseline_2_BeforeQMIX.png\" width=\"900\" height=\"600\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url=\"Baseline_2_BeforeQMIX.png\", width=900, height=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e650fa21",
   "metadata": {},
   "source": [
    "<h2> QMIX</h2>\n",
    "Second attempt at baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5dbd7c4",
   "metadata": {},
   "source": [
    "First Attempt:\n",
    "\n",
    "ep_length_mean:          296.0036       epsilon:                   0.0500       grad_norm:               44408.9617     hold_mean:                 0.2679\n",
    "loss:                    15895.5070     q_taken_mean:              8.6338       return_mean:             1068.4453      return_std:              1801.3856\n",
    "success_mean:              0.0536       t_mean:                  296.0036       target_mean:               8.5500       td_error_abs:             21.6396\n",
    "test_ep_length_mean:     262.0500       test_hold_mean:            0.7500       test_return_mean:        2047.0497      test_return_std:         2918.8472\n",
    "test_success_mean:         0.1500       test_t_mean:             262.0500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4f0c7fac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"BaselineFirstQMIX.png\" width=\"900\" height=\"600\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import image module\n",
    "from IPython.display import Image\n",
    "\n",
    "# get the image\n",
    "Image(url=\"BaselineFirstQMIX.png\", width=900, height=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b009dd",
   "metadata": {},
   "source": [
    "I notice that gradient is EXPLODING and loss is huge. I change my reward structure to make it smoother and more normalized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d15c8e1",
   "metadata": {},
   "source": [
    "Second Attempt:\n",
    "ep_length_mean:          250.6667       epsilon:                   0.0500       grad_norm:                 1.3428       hold_mean:                 1.2222\n",
    "loss:                      0.7356       q_taken_mean:              0.2904       return_mean:             -248.2167      return_std:              520.8183\n",
    "success_mean:              0.2444       t_mean:                  250.6667       target_mean:               0.2837       td_error_abs:              0.1454\n",
    "test_ep_length_mean:     271.5500       test_hold_mean:            0.7500       test_return_mean:        -160.0374      test_return_std:         573.6516"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d18f4c48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"QMIX2ndBaseline.png\" width=\"900\" height=\"600\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url=\"QMIX2ndBaseline.png\", width=900, height=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e62fa2c",
   "metadata": {},
   "source": [
    "Clearly doing much better and trending in correct direction. \n",
    "We see that QMIX is doing much better, and a better reward structure is paying off. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa830052",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make distance shaping even better : exponential pull towards ghost    def AgentToGhostDist_Reward(self, prev_agent_ghost_d):\n",
    "\n",
    "def AgentToGhostDist_Reward(self, prev_agent_ghost_d):\n",
    "    reward = 0.0\n",
    "\n",
    "    # Closer to ghost = higher reward every step\n",
    "    max_dist = max(self.grid.width, self.grid.height) - 1\n",
    "    \n",
    "    for a in self.agents:\n",
    "        dist = cheb_dist((a.x, a.y), self.ghostCoords)\n",
    "        norm_dist = dist / max_dist\n",
    "        reward += self.lambda_agent_dist_to_ghost * ((1.0 - norm_dist) ** 2) * 2.0 #exponential pull for stronger incentive \n",
    "\n",
    "    return reward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "711a062f",
   "metadata": {},
   "source": [
    "<h2> BOOM here we are </h2>\n",
    "\n",
    "[INFO 11:05:27] my_main Recent Stats | t_env:     140057 | Episode:     1484\n",
    "ep_length_mean:           30.6744       epsilon:                   0.3431       grad_norm:                31.0257       hold_mean:                 5.0000\n",
    "loss:                      1.9014       q_taken_mean:              2.5236       return_mean:             320.9782       return_std:               71.2775\n",
    "success_mean:              1.0000       t_mean:                   30.6744       target_mean:               2.4942       td_error_abs:              0.7269\n",
    "test_ep_length_mean:      32.1500       test_hold_mean:            5.0000       test_return_mean:        365.2984       test_return_std:         108.3838\n",
    "test_success_mean:         1.0000       test_t_mean:              32.1500\n",
    "\n",
    "\n",
    "Exponential shaping made a huuuuuuge difference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d097f5",
   "metadata": {},
   "source": [
    "Proof that VDN could not do this: \n",
    "Same env, rewards, etc, just with vdn\n",
    "\n",
    "[INFO 11:09:27] my_main Recent Stats | t_env:     198294 | Episode:      672\n",
    "ep_length_mean:          295.8000       epsilon:                   0.0500       grad_norm:               112.0349       hold_mean:                 0.1250\n",
    "loss:                     10.1877       q_taken_mean:              3.3301       return_mean:             762.8890       return_std:              271.7277\n",
    "success_mean:              0.0250       t_mean:                  295.8000       target_mean:               3.3562       td_error_abs:              1.4568\n",
    "test_ep_length_mean:     300.0000       test_hold_mean:            0.0000       test_return_mean:        897.3207       test_return_std:         192.5651\n",
    "test_success_mean:         0.0000       test_t_mean:             300.0000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bded03a",
   "metadata": {},
   "source": [
    "<h1>Stage 2: Capturing a moving ghost</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff6e8cf4",
   "metadata": {},
   "source": [
    "Ghost now moves 30% of the time, with a 20% decrease for every agent near it\n",
    "Spawns further away : 15"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead21f1f",
   "metadata": {},
   "source": [
    "First attempt gives 35% success rate. Its ok but not great. We want this to improve\n",
    "We test the policy (QMIX_Stage2_1_) and notice that they quickly get to it and close to it, but can struggle to fully surround and capture.\n",
    "I notice ghosts tend to enter surround and leave again and again, so I will strongly reward staying in surround radius, and decay the bonus for a new complete surround.\n",
    "Also will encourage coverage on multiple directions/quadrants of the ghost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "05701b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def globalReward(self, prev_d_ghost_extract, prev_d_ghost_agents, prev_surround_count):\n",
    "        reward = -0.01  # Tiny time penalty to encourage efficiency (won't dominate)\n",
    "        terminated = False\n",
    "        success = False\n",
    "\n",
    "        if self.surroundCounter >= numAgents:\n",
    "            self.holdCounter += 1\n",
    "        else:\n",
    "            self.holdCounter = 0\n",
    "\n",
    "        # End conditions\n",
    "        if self.holdCounter >= self.timeToKill:\n",
    "            reward += self.win_reward  # HUGE success reward to make it unmissable\n",
    "            terminated = True\n",
    "            success = True\n",
    "        elif self.Time >= self.episodeLimit:\n",
    "            terminated = True\n",
    "            # NO massive failure penalty - let shaped rewards guide learning\n",
    "\n",
    "        if terminated:\n",
    "            return reward, terminated, success\n",
    "        \n",
    "\n",
    "        # Reward for getting agents closer to ghost\n",
    "        reward += self.AgentToGhostDist_Reward(prev_d_ghost_agents)\n",
    "\n",
    "        #Reward for surrounding ghost\n",
    "        reward += self.Surround_Reward(prev_surround_count)\n",
    "        \n",
    "        # Reward for spreading around ghost at different angles\n",
    "        if self.surroundCounter >= 2:  # Only when at least 2 agents nearby\n",
    "            reward += self.QuadrantCoverage_Reward()\n",
    "\n",
    "        #penalty for agents too close to each other (when NOT surrounding ghost)\n",
    "        if self.surroundCounter < numAgents:\n",
    "            pass\n",
    "            reward += self.AgentClumpingPenalty()\n",
    "\n",
    "\n",
    "        return reward, terminated, success\n",
    "\n",
    "\n",
    "def QuadrantCoverage_Reward(self):\n",
    "        \"\"\"\n",
    "        Reward agents for surrounding ghost from different cardinal directions.\n",
    "        Checks if agents are positioned left, right, above, or below the ghost.\n",
    "        Rewards for each direction covered.\n",
    "        \"\"\"\n",
    "        \n",
    "        gx, gy = self.ghostCoords\n",
    "        directions = [False, False, False, False]  # Left, Right, Above, Below\n",
    "        \n",
    "        for ax, ay in self.agentCoords:\n",
    "            # Only count agents within surround radius\n",
    "            if cheb_dist((ax, ay), (gx, gy)) <= self.surroundRadius:\n",
    "                dx = ax - gx\n",
    "                dy = ay - gy\n",
    "                \n",
    "                # Determine primary direction (using dominance of dx vs dy)\n",
    "                if abs(dx) > abs(dy):  # Horizontal dominance\n",
    "                    if dx > 0:\n",
    "                        directions[1] = True  # Right\n",
    "                    else:\n",
    "                        directions[0] = True  # Left\n",
    "                else:  # Vertical dominance\n",
    "                    if dy > 0:\n",
    "                        directions[2] = True  # Above\n",
    "                    else:\n",
    "                        directions[3] = True  # Below\n",
    "        \n",
    "        # Reward per direction covered\n",
    "        num_covered = sum(directions)\n",
    "        return self.lambda_quadrant_coverage * num_covered"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c055d642",
   "metadata": {},
   "source": [
    "From visual testing what is happening is that they get close very quickly, but struggle to fully surround and capture. It seems like agents get in the way, and possibly exploit coming in and out of surround radius\n",
    "\n",
    "ALSO: Results great until 200k steps, then things break. Seems like issues with epsilon: exploration etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4144abb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"QMIX_Stage2_AfterQuadrantsAndDecay.png\" width=\"900\" height=\"600\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url=\"QMIX_Stage2_AfterQuadrantsAndDecay.png\", width=900, height=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0fd4958",
   "metadata": {},
   "source": [
    "At this point I decided to try stage 3 and basically same rewards and same behaviour. \n",
    "I notice once agents get close to ghost, they behave nearly identically. \n",
    "\n",
    "So I'm going to try enabling adding agent id to observations to allow for more heterogenous actions and more \"separation\" between the agents. \n",
    "I Cubed the distance so they wanna get even closer\n",
    "Then I made the avoid radius smaller"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b04980ad",
   "metadata": {},
   "source": [
    "NOTHING was working. So I had to dig deeper to analyze and given their erratic behaviour, was suspucious of an underlying problem so I looked at step how movement was working. This lead me to step(). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8078944a",
   "metadata": {},
   "source": [
    "\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24226b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Before\n",
    "\n",
    "# 1. Agents act\n",
    "for a, act_id in zip(self.agents, actions):\n",
    "    a.apply_action(act_id)\n",
    "\n",
    "# 2. Ghost moves (AFTER agents)\n",
    "self.ghostCoords = self.ghost.move(self.agentCoords)\n",
    "\n",
    "# 3. Calculate rewards based on NEW positions\n",
    "reward = self.globalReward(...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "764ad93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Ghost moves FIRST\n",
    "self.ghostCoords = self.ghost.move(self.agentCoords)\n",
    "\n",
    "# 2. Agents act (seeing where ghost went)\n",
    "for a, act_id in zip(self.agents, actions):\n",
    "    a.apply_action(act_id)\n",
    "\n",
    "# 3. Calculate rewards\n",
    "reward = self.globalReward(...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352ac26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(url=\"QMIX_Stage3_UpdatedStep.png\", width=900, height=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c68fd168",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def AgentToGhostDist_Reward(self, prev_agent_ghost_d):\n",
    "        reward = 0.0\n",
    "\n",
    "        # Closer to ghost = higher reward every step\n",
    "        # Use Chebyshev distance to match surround logic!\n",
    "        max_dist = max(self.grid.width - 1, self.grid.height - 1)\n",
    "        \n",
    "        for a in self.agents:\n",
    "            dist = cheb_dist((a.x, a.y), self.ghostCoords)\n",
    "            norm_dist = dist / max_dist\n",
    "            # STRONGER exponential: cube instead of square for aggressive pull when close\n",
    "            reward += self.lambda_agent_dist_to_ghost * ((1.0 - norm_dist) ** 3) * 3.0\n",
    "\n",
    "        return reward\n",
    "    \n",
    "    def Surround_Reward(self, prev_surround_count):\n",
    "        reward = 0.0\n",
    "        new_surround_count = self.ghost.GetSurroundedCount(self.agentCoords)\n",
    "\n",
    "        # Progressive surround rewards - BIG bonus for entering radius\n",
    "        if new_surround_count > prev_surround_count:\n",
    "            reward += self.reward_new_surround * (new_surround_count - prev_surround_count) * 2.0\n",
    "        \n",
    "        # PENALTY: Breaking formation (agents leaving surround) - REDUCED to encourage commitment\n",
    "        if new_surround_count < prev_surround_count:\n",
    "            penalty = self.penalty_formation_break * (prev_surround_count - new_surround_count) * 0.5\n",
    "            reward -= penalty\n",
    "        \n",
    "        \n",
    "        # Keeping full surround\n",
    "        if new_surround_count >= numAgents:\n",
    "            reward += self.reward_full_surround\n",
    "\n",
    "        return reward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4712c056",
   "metadata": {},
   "source": [
    "<h1> Stage 4: Extraction <h1>\n",
    "We change the game: now must extract."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01901dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be86ab6d",
   "metadata": {},
   "source": [
    "First attempts fail. \n",
    "\n",
    "Not going well. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c81ec753",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "        #For shaping rewards per phase of game\n",
    "        surround_progress = self.surroundCounter / numAgents  # 0.0 â†’ 1.0\n",
    "        \n",
    "        extraction_multiplier = 0.1 + 1.9 * (surround_progress ** 3)\n",
    "        surrounding_multiplier = 1.0\n",
    "\n",
    "        #STEP1: Surrounding\n",
    "        reward += surrounding_multiplier * self.AgentToGhostDist_Reward(prev_d_ghost_agents)\n",
    "        reward += surrounding_multiplier * self.Surround_Reward(prev_surround_count)\n",
    "        reward += surrounding_multiplier * self.QuadrantCoverage_Reward()\n",
    "\n",
    "        #Step2: Extraction\n",
    "        reward += extraction_multiplier * self.GhostToExtraction_Reward(prev_d_ghost_extract)\n",
    "\n",
    "        return reward, terminated, success"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a84943",
   "metadata": {},
   "source": [
    "Now I will heavily reward not (only) surrounding, but leaving a hole aka pushing on the correct side. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "262cc3e1",
   "metadata": {},
   "source": [
    "so we would have to computer the required direction for ghost to go, then reward the agent's position for aligning with this, but from the other direciotn\n",
    "\n",
    "I'm thiking we could find the general line of the agents, and take the dot product with the ideal one. This tells us how close we are to that, and we can reward this. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f89cd3",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
