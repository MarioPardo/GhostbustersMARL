{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23c83143",
   "metadata": {},
   "source": [
    "<h1> Project Progress </h1>\n",
    "\n",
    "This file will track the progress of this project, including major steps, code changes, results, etc. \n",
    "A ipynb file is best as it will allow for code, writing, etc and best formatting. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa38aa37",
   "metadata": {},
   "source": [
    "<h1> Baseline: Full Observability: Catch only <h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca00c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Observations\n",
    "\n",
    "def getAgentObs(self,agentIndex):\n",
    "    W, H = self.grid.width, self.grid.height\n",
    "    ax, ay = self.agents[agentIndex].x, self.agents[agentIndex].y\n",
    "    gx, gy = self.ghostCoords\n",
    "\n",
    "    numAgents = len(self.agents)\n",
    "    obs = [\n",
    "        ax / (W-1), ay / (H-1),\n",
    "        gx / (W-1), gy / (H-1),\n",
    "        self.grid.extraction_area_tl[0]/(W-1), self.grid.extraction_area_tl[1]/(H-1),\n",
    "        self.grid.extraction_area_br[0]/(W-1), self.grid.extraction_area_br[1]/(H-1),\n",
    "\n",
    "        #Show Progress\n",
    "        cheb_dist((ax, ay), (gx, gy)) / max(W-1, H-1),\n",
    "        self.surroundCounter / numAgents,\n",
    "        self.holdCounter / self.timeToKill\n",
    "    ]\n",
    "\n",
    "    return obs\n",
    "\n",
    "\n",
    "#State\n",
    "\n",
    "def get_state(self):\n",
    "\n",
    "    W, H = self.gridwidth, self.gridheight\n",
    "    s = []\n",
    "\n",
    "    # all agents\n",
    "    for a in self.engine.agents:\n",
    "        s += [a.x/(W-1), a.y/(H-1)]\n",
    "\n",
    "    #Ghost + visible flag (full obs baseline â†’ 1.0)\n",
    "    gx, gy = self.engine.ghostCoords\n",
    "    s += [gx/(W-1), gy/(H-1), 1.0]\n",
    "\n",
    "    #Extraction area\n",
    "    etlx, etly = self.engine.grid.extraction_area_tl\n",
    "    ebrx, ebry = self.engine.grid.extraction_area_br\n",
    "    s += [etlx/(W-1), etly/(H-1), ebrx/(W-1), ebry/(H-1)]\n",
    "\n",
    "    #Game progress\n",
    "    s += [self.engine.Time / self.episode_limit]\n",
    "\n",
    "    return np.asarray(s, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b18a7c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rewards\n",
    "\n",
    "\n",
    "    def globalReward(self, prev_d_ghost_extract, prev_d_ghost_agents, prev_surround_count):\n",
    "        reward = -0.01 \n",
    "        terminated = False\n",
    "        success = False\n",
    "\n",
    "        if self.isSurrounded():\n",
    "            self._hold_counter += 1\n",
    "        if self.surroundCounter >= numAgents:\n",
    "            self.holdCounter += 1\n",
    "        else:\n",
    "            self._hold_counter = 0\n",
    "            self.holdCounter = 0\n",
    "\n",
    "        terminated = False\n",
    "     \n",
    "        # End conditions\n",
    "        if self.holdCounter >= self.timeToKill:\n",
    "            reward += self.win_reward  \n",
    "            terminated = True\n",
    "            success = True\n",
    "        elif self.Time >= self.episodeLimit:\n",
    "            terminated = True\n",
    "            reward -= 1000.0  \n",
    "\n",
    "        if terminated:\n",
    "            return reward, terminated, success\n",
    "        \n",
    "        #Reward for surrounding ghost\n",
    "        reward += self.Surround_Reward(prev_surround_count)\n",
    "\n",
    "\n",
    "        return reward, terminated, success\n",
    "\n",
    "\n",
    "def AgentClumpingPenalty(self):\n",
    "        penalty = 0.0\n",
    "        CLUMP_DISTANCE = 2\n",
    "        \n",
    "        # Only penalize if not currently surrounding ghost\n",
    "        if self.surroundCounter >= numAgents - 1:\n",
    "            return 0.0  # Allow clustering when capturing\n",
    "        \n",
    "        for i in range(len(self.agents)):\n",
    "            for j in range(i + 1, len(self.agents)):\n",
    "                dist = cheb_dist((self.agents[i].x, self.agents[i].y), \n",
    "                                 (self.agents[j].x, self.agents[j].y))\n",
    "               \n",
    "                if dist <= CLUMP_DISTANCE:\n",
    "                    penalty += 0.1  \n",
    "        \n",
    "        return penalty\n",
    "\n",
    "\n",
    " def Surround_Reward(self, prev_surround_count):\n",
    "        reward = 0.0\n",
    "        new_surround_count = self.ghost.GetSurroundedCount(self.agentCoords)\n",
    "\n",
    "        # Progressive surround rewards - each agent joining is valuable\n",
    "        if new_surround_count > prev_surround_count:\n",
    "            # Increasing reward per agent (more agents = more valuable)\n",
    "            reward += self.reward_new_surround * (new_surround_count - prev_surround_count)\n",
    "\n",
    "\n",
    "        # Bonus for achieving full surround (all agents within radius)\n",
    "        if new_surround_count >= numAgents:\n",
    "            reward += self.reward_surrounded\n",
    "\n",
    "        return reward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669c5e33",
   "metadata": {},
   "source": [
    "<h2> Stage 1 </h2>\n",
    "Hold time: 3 step\n",
    "No extraction\n",
    "No ghost movement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36fecd90",
   "metadata": {},
   "source": [
    "<h2> VDN </h2>\n",
    "Did not work. This was the last and best attempt\n",
    "<pre>\n",
    "[INFO 10:16:01] my_main Recent Stats | t_env:     199894 | Episode:      676\n",
    "ep_length_mean:          300.0000       epsilon:                   0.0500       grad_norm:               2968.1982      hold_mean:                 0.0000\n",
    "loss:                    9660.9391      q_taken_mean:              4.3232       return_mean:             462.8749       return_std:              980.8482\n",
    "success_mean:              0.0000       t_mean:                  300.0000       target_mean:               5.4364       td_error_abs:             10.1837\n",
    "test_ep_length_mean:     286.3500       test_hold_mean:            0.2500       test_return_mean:        841.0543       test_return_std:         1298.6543\n",
    "test_success_mean:         0.0500       test_t_mean:             286.3500\n",
    "[INFO 10:16:01] my_main Finished Training\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "af7e7d8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"ResultImages/Baseline_2_BeforeQMIX.png\" width=\"900\" height=\"600\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url=\"ResultImages/Baseline_2_BeforeQMIX.png\", width=900, height=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3984f65",
   "metadata": {},
   "source": [
    "VDN Seems to not be able to model the collaboration needed for this task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e650fa21",
   "metadata": {},
   "source": [
    "<h2> QMIX</h2>\n",
    "Second attempt at baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5dbd7c4",
   "metadata": {},
   "source": [
    "First Attempt:\n",
    "\n",
    "<pre> \n",
    "\n",
    "ep_length_mean:          296.0036       epsilon:                   0.0500       grad_norm:               44408.9617     hold_mean:                 0.2679\n",
    "loss:                    15895.5070     q_taken_mean:              8.6338       return_mean:             1068.4453      return_std:              1801.3856\n",
    "success_mean:              0.0536       t_mean:                  296.0036       target_mean:               8.5500       td_error_abs:             21.6396\n",
    "test_ep_length_mean:     262.0500       test_hold_mean:            0.7500       test_return_mean:        2047.0497      test_return_std:         2918.8472\n",
    "test_success_mean:         0.1500       test_t_mean:             262.0500\n",
    "\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4f0c7fac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"ResultImages/BaselineFirstQMIX.png\" width=\"900\" height=\"600\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import image module\n",
    "from IPython.display import Image\n",
    "\n",
    "# get the image\n",
    "Image(url=\"ResultImages/BaselineFirstQMIX.png\", width=900, height=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b009dd",
   "metadata": {},
   "source": [
    "I notice that gradient is EXPLODING and loss is huge. I change my reward structure to make it smoother and more normalized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d15c8e1",
   "metadata": {},
   "source": [
    "Second Attempt:\n",
    "ep_length_mean:          250.6667       epsilon:                   0.0500       grad_norm:                 1.3428       hold_mean:                 1.2222\n",
    "loss:                      0.7356       q_taken_mean:              0.2904       return_mean:             -248.2167      return_std:              520.8183\n",
    "success_mean:              0.2444       t_mean:                  250.6667       target_mean:               0.2837       td_error_abs:              0.1454\n",
    "test_ep_length_mean:     271.5500       test_hold_mean:            0.7500       test_return_mean:        -160.0374      test_return_std:         573.6516"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d18f4c48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"ResultImages/QMIX2ndBaseline.png\" width=\"900\" height=\"600\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url=\"ResultImages/QMIX2ndBaseline.png\", width=900, height=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e62fa2c",
   "metadata": {},
   "source": [
    "Clearly doing much better and trending in correct direction. \n",
    "We see that QMIX is doing much better, and a better reward structure is paying off. \n",
    "\n",
    "We are going in the right direction, now that we see some structure and learning, let's tweak our reward structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa830052",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make distance shaping even better : exponential pull towards ghost   \n",
    "\n",
    "def AgentToGhostDist_Reward(self, prev_agent_ghost_d):\n",
    "    reward = 0.0\n",
    "\n",
    "    # Closer to ghost = higher reward every step\n",
    "    max_dist = max(self.grid.width, self.grid.height) - 1\n",
    "    \n",
    "    for a in self.agents:\n",
    "        dist = cheb_dist((a.x, a.y), self.ghostCoords)\n",
    "        norm_dist = dist / max_dist\n",
    "        reward += self.lambda_agent_dist_to_ghost * ((1.0 - norm_dist) ** 2) * 2.0 \n",
    "\n",
    "    return reward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e0e416",
   "metadata": {},
   "source": [
    "We still get some Q exploitation behaviour, so I add progress markers into state so Q functions can better estimate value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7883c159",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def get_state(self):\n",
    "        \"\"\"\n",
    "        Previous stuff\n",
    "        \"\"\"\n",
    "\n",
    "        #Global task progress (for value estimation)\n",
    "        s += [\n",
    "            self.engine.surroundCounter / self.n_agents,  # surround progress [0-1]\n",
    "            # Average distance to ghost (normalized)\n",
    "            sum(cheb_dist((a.x, a.y), self.engine.ghostCoords) for a in self.engine.agents) / (self.n_agents * max(W-1, H-1))\n",
    "        ]\n",
    "\n",
    "        return np.asarray(s, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "711a062f",
   "metadata": {},
   "source": [
    "<h2> BOOM here we are </h2>\n",
    "\n",
    "<pre>\n",
    "\n",
    "[INFO 11:05:27] my_main Recent Stats | t_env:     140057 | Episode:     1484\n",
    "ep_length_mean:           30.6744       epsilon:                   0.3431       grad_norm:                31.0257       hold_mean:                 5.0000\n",
    "loss:                      1.9014       q_taken_mean:              2.5236       return_mean:             320.9782       return_std:               71.2775\n",
    "success_mean:              1.0000       t_mean:                   30.6744       target_mean:               2.4942       td_error_abs:              0.7269\n",
    "test_ep_length_mean:      32.1500       test_hold_mean:            5.0000       test_return_mean:        365.2984       test_return_std:         108.3838\n",
    "test_success_mean:         1.0000       test_t_mean:              32.1500\n",
    "</pre>\n",
    "\n",
    "Exponential shaping made a huuuuuuge difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "88baeea8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"ResultImages/WorkingQMIXBaseline.png\" width=\"900\" height=\"600\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url=\"ResultImages/WorkingQMIXBaseline.png\", width=900, height=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d097f5",
   "metadata": {},
   "source": [
    "<h3> Wait: Maybe it wasn't VDN's fault? </h3>\n",
    "\n",
    "Proof that VDN could not do this: \n",
    "Same env, rewards, etc, just with vdn\n",
    "\n",
    "<pre>\n",
    "\n",
    "[INFO 11:09:27] my_main Recent Stats | t_env:     198294 | Episode:      672\n",
    "ep_length_mean:          295.8000       epsilon:                   0.0500       grad_norm:               112.0349       hold_mean:                 0.1250\n",
    "loss:                     10.1877       q_taken_mean:              3.3301       return_mean:             762.8890       return_std:              271.7277\n",
    "success_mean:              0.0250       t_mean:                  295.8000       target_mean:               3.3562       td_error_abs:              1.4568\n",
    "test_ep_length_mean:     300.0000       test_hold_mean:            0.0000       test_return_mean:        897.3207       test_return_std:         192.5651\n",
    "test_success_mean:         0.0000       test_t_mean:             300.0000\n",
    "\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bded03a",
   "metadata": {},
   "source": [
    "<h1>Stage 2: Capturing a moving ghost</h1>\n",
    "Great we can capture a static ghost. Now let's make the ghost move and make this task much harder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff6e8cf4",
   "metadata": {},
   "source": [
    "Ghost now moves 30% of the time, with a 20% decrease for every agent near it\n",
    "Spawns further away : 15"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead21f1f",
   "metadata": {},
   "source": [
    "First attempt gives 35% success rate. Its ok but not great. We want this to improve <br>\n",
    "We test the policy and notice that they quickly get to it and close to it, but can struggle to fully surround and capture.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "99db6c57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video controls  >\n",
       " <source src=\"data:None;base64,QMIX_FullObv_Stage2_CantCatch.mov\" type=\"None\">\n",
       " Your browser does not support the video tag.\n",
       " </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Video\n",
    "\n",
    "Video(\"QMIX_FullObv_Stage2_CantCatch.mov\", embed=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da29d5e2",
   "metadata": {},
   "source": [
    "I notice ghosts tend to enter surround and leave again and again, so I will strongly reward staying in surround radius, and decay the bonus for a new complete surround.\n",
    "Also will encourage coverage on multiple directions/quadrants of the ghost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05701b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def globalReward(self, prev_d_ghost_extract, prev_d_ghost_agents, prev_surround_count):\n",
    "        reward = -0.01  \n",
    "        terminated = False\n",
    "        success = False\n",
    "\n",
    "        if self.surroundCounter >= numAgents:\n",
    "            self.holdCounter += 1\n",
    "        else:\n",
    "            self.holdCounter = 0\n",
    "\n",
    "        # End conditions\n",
    "        if self.holdCounter >= self.timeToKill:\n",
    "            reward += self.win_reward  # HUGE success reward to make it unmissable\n",
    "            terminated = True\n",
    "            success = True\n",
    "        elif self.Time >= self.episodeLimit:\n",
    "            terminated = True\n",
    "\n",
    "\n",
    "        if terminated:\n",
    "            return reward, terminated, success\n",
    "        \n",
    "        # Reward for getting agents closer to ghost\n",
    "        reward += self.AgentToGhostDist_Reward(prev_d_ghost_agents)\n",
    "\n",
    "        #Reward for surrounding ghost\n",
    "        reward += self.Surround_Reward(prev_surround_count)\n",
    "\n",
    "        if self.surroundCounter >= 2:  # Only when at least 2 agents nearby\n",
    "            reward += self.QuadrantCoverage_Reward()\n",
    "\n",
    "        if self.surroundCounter < numAgents:  #when NOT surrounding ghost)\n",
    "            reward += self.AgentClumpingPenalty()\n",
    "\n",
    "        return reward, terminated, success\n",
    "\n",
    "\n",
    "def QuadrantCoverage_Reward(self):\n",
    "        \n",
    "        gx, gy = self.ghostCoords\n",
    "        directions = [False, False, False, False]  # Left, Right, Above, Below\n",
    "        \n",
    "        for ax, ay in self.agentCoords:\n",
    "            # Only count agents within surround radius\n",
    "            if cheb_dist((ax, ay), (gx, gy)) <= self.surroundRadius:\n",
    "                dx = ax - gx\n",
    "                dy = ay - gy\n",
    "                \n",
    "                # Determine primary direction (using dominance of dx vs dy)\n",
    "                if abs(dx) > abs(dy):  # Horizontal dominance\n",
    "                    if dx > 0:\n",
    "                        directions[1] = True  # Right\n",
    "                    else:\n",
    "                        directions[0] = True  # Left\n",
    "                else:  # Vertical dominance\n",
    "                    if dy > 0:\n",
    "                        directions[2] = True  # Above\n",
    "                    else:\n",
    "                        directions[3] = True  # Below\n",
    "        \n",
    "        # Reward per direction covered\n",
    "        num_covered = sum(directions)\n",
    "        return self.lambda_quadrant_coverage * num_covered"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c055d642",
   "metadata": {},
   "source": [
    "From visual testing what is happening is that they get close very quickly, but struggle to fully surround and capture. It seems like agents get in the way, and possibly exploit coming in and out of surround radius"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd900c9",
   "metadata": {},
   "source": [
    "<h3> Weird Training Behaviour</h3>\n",
    "\n",
    "Results great until 200k steps, then things break. Seems like issues with epsilon: exploration etc.\n",
    "So the reward structure is working well, but now likely issues with learning algo details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4144abb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"ResultImages/QMIX_Stage2_BrokenAt200k.png\" width=\"900\" height=\"600\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url=\"ResultImages/QMIX_Stage2_BrokenAt200k.png\", width=900, height=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0fd4958",
   "metadata": {},
   "source": [
    "At this point I decided to try stage 3 and basically same rewards and same behaviour. \n",
    "\n",
    "So I'm going to try enabling adding agent id to observations to allow for more heterogenous actions and more \"separation\" between the agents. \n",
    "I Cubed the distance so they wanna get even closer\n",
    "Then I made the avoid radius smaller"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b04980ad",
   "metadata": {},
   "source": [
    "<h1> BIG Bug found: Incorrect order of rewards </h1>\n",
    "NOTHING was working. So I had to dig deeper to analyze and given their erratic behaviour.\n",
    "Was suspucious of an underlying problem so I looked at step how movement was working. This lead me to step(). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24226b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Before\n",
    "# 1. Agents act\n",
    "for a, act_id in zip(self.agents, actions):\n",
    "    a.apply_action(act_id)\n",
    "\n",
    "# 2. Ghost moves (AFTER agents)\n",
    "self.ghostCoords = self.ghost.move(self.agentCoords)\n",
    "\n",
    "# 3. Calculate rewards based on NEW positions\n",
    "reward = self.globalReward(...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "764ad93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#After\n",
    "self.ghostCoords = self.ghost.move(self.agentCoords)\n",
    "\n",
    "# 2. Agents act (seeing where ghost went)\n",
    "for a, act_id in zip(self.agents, actions):\n",
    "    a.apply_action(act_id)\n",
    "\n",
    "# 3. Calculate rewards\n",
    "reward = self.globalReward(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c96dc13",
   "metadata": {},
   "source": [
    "Before, we had action -> state change -> reward\n",
    "\n",
    "What we SHOULD have is state change -> act accordingly -> reward\n",
    "So we had a small but very important missmatch on what we were doing vs getting rewarded for\n",
    "\n",
    "Why did we not notice this earlier? The Ghost wasn't moving, so it didnt update the state. Now only that the ghost moves and we depend on it's movement, did we notice\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "352ac26d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"ResultImages/QMIX_Stage3_UpdatedStep.png\" width=\"900\" height=\"600\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url=\"ResultImages/QMIX_Stage3_UpdatedStep.png\", width=900, height=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c4e882a",
   "metadata": {},
   "source": [
    "Amazing. Now for some final reward shaping additions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c68fd168",
   "metadata": {},
   "outputs": [],
   "source": [
    "def AgentToGhostDist_Reward(self, prev_agent_ghost_d):\n",
    "        reward = 0.0\n",
    "\n",
    "        max_dist = max(self.grid.width - 1, self.grid.height - 1)\n",
    "        \n",
    "        for a in self.agents:\n",
    "            dist = cheb_dist((a.x, a.y), self.ghostCoords)\n",
    "            norm_dist = dist / max_dist\n",
    "            # STRONGER exponential: cube instead of square for aggressive pull when close\n",
    "            reward += self.lambda_agent_dist_to_ghost * ((1.0 - norm_dist) ** 3) * 3.0\n",
    "\n",
    "        return reward\n",
    "    \n",
    "def Surround_Reward(self, prev_surround_count):\n",
    "        reward = 0.0\n",
    "        new_surround_count = self.ghost.GetSurroundedCount(self.agentCoords)\n",
    "\n",
    "        # Progressive surround rewards - BIG bonus for entering radius\n",
    "        if new_surround_count > prev_surround_count:\n",
    "            reward += self.reward_new_surround * (new_surround_count - prev_surround_count) * 2.0\n",
    "        \n",
    "        # Penalty Breaking formation\n",
    "        if new_surround_count < prev_surround_count:\n",
    "            penalty = self.penalty_formation_break * (prev_surround_count - new_surround_count) * 0.5\n",
    "            reward -= penalty\n",
    "        \n",
    "        \n",
    "        # Keeping full surround\n",
    "        if new_surround_count >= numAgents:\n",
    "            reward += self.reward_full_surround\n",
    "\n",
    "        return reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ed1a7955",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"ResultImages/WorkingQMIXBaseline.png\" width=\"900\" height=\"600\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url=\"ResultImages/WorkingQMIXBaseline.png\", width=900, height=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4712c056",
   "metadata": {},
   "source": [
    "<h1> Stage 3: Extraction </h1>\n",
    "We move closer to our end goal of catching -> move to extraction sites"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be86ab6d",
   "metadata": {},
   "source": [
    "First attempts fail. \n",
    "\n",
    "Not going well. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c81ec753",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#For shaping rewards per phase of game\n",
    "surround_progress = self.surroundCounter / numAgents \n",
    "\n",
    "extraction_multiplier = 0.1 + 1.9 * (surround_progress ** 3)\n",
    "surrounding_multiplier = 1.0\n",
    "\n",
    "#STEP1: Surrounding\n",
    "reward += surrounding_multiplier * self.AgentToGhostDist_Reward(prev_d_ghost_agents)\n",
    "reward += surrounding_multiplier * self.Surround_Reward(prev_surround_count)\n",
    "reward += surrounding_multiplier * self.QuadrantCoverage_Reward()\n",
    "\n",
    "#Step2: Extraction\n",
    "reward += extraction_multiplier * self.GhostToExtraction_Reward(prev_d_ghost_extract)\n",
    "\n",
    "return reward, terminated, success"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a84943",
   "metadata": {},
   "source": [
    "Now I will heavily reward not (only) surrounding, but leaving a hole aka pushing on the correct side. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "262cc3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def DirectionalPush_Reward(self):\n",
    "    #Reward agents for positioning to PUSH the ghost toward extraction.\n",
    "\n",
    "    gx, gy = self.ghostCoords\n",
    "    ex, ey = self.grid.extraction_point_center\n",
    "    \n",
    "    # Vector from ghost to extraction (desired direction for ghost to move)\n",
    "    ghost_to_extract_x = ex - gx\n",
    "    ghost_to_extract_y = ey - gy\n",
    "    extract_dist = max(abs(ghost_to_extract_x), abs(ghost_to_extract_y))  # Chebyshev distance\n",
    "    \n",
    "    # Avoid division by zero if ghost is at extraction\n",
    "    if extract_dist < 0.1:\n",
    "        return 0.0\n",
    "    \n",
    "    # Normalize the extraction direction vector\n",
    "    extract_dir_x = ghost_to_extract_x / extract_dist\n",
    "    extract_dir_y = ghost_to_extract_y / extract_dist\n",
    "    \n",
    "    total_reward = 0.0\n",
    "    max_dist = max(self.grid.width, self.grid.height) - 1\n",
    "    \n",
    "    for ax, ay in self.agentCoords:\n",
    "        ghost_to_agent_x = ax - gx\n",
    "        ghost_to_agent_y = ay - gy\n",
    "        agent_dist = cheb_dist((ax, ay), (gx, gy))\n",
    "        \n",
    "        \n",
    "        # Normalize agent direction vector\n",
    "        agent_dir_x = ghost_to_agent_x / max(agent_dist, 1)\n",
    "        agent_dir_y = ghost_to_agent_y / max(agent_dist, 1)\n",
    "        \n",
    "        # Dot product: want agents opposite extraction direction, so we want negative\n",
    "        dot_product = (agent_dir_x * extract_dir_x + agent_dir_y * extract_dir_y)\n",
    "        \n",
    "        # Calculate proximity bonus. Reward when close\n",
    "        normalized_dist = agent_dist / max_dist\n",
    "        proximity_multiplier = max(0, (1.0 - normalized_dist)) ** 1.5 \n",
    "        \n",
    "        # Simple exponential reward: maximize alignment opposite to extraction direction\n",
    "        alignment_score = (-dot_product + 1) / 2  # Maps [-1, 1] to [1.0, 0.0]\n",
    "        directional_reward = self.lambda_quadrant_coverage * proximity_multiplier * (alignment_score ** 3)\n",
    "        total_reward += directional_reward  \n",
    "\n",
    "    return total_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f89cd3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "708d52c3",
   "metadata": {},
   "source": [
    "<h2> EXTRACTION DID NOT WORK </h2>\n",
    "\n",
    "Accept defeat, reframe final goal. We will just be catching under partial observability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24fe4de8",
   "metadata": {},
   "source": [
    "<h1> Catching Only: Partial Observability </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fbb6862",
   "metadata": {},
   "outputs": [],
   "source": [
    "#New Observations\n",
    "\n",
    "def getAgentObs(self, agentIndex):\n",
    "    W, H = self.grid.width, self.grid.height\n",
    "    ax, ay = self.agents[agentIndex].x, self.agents[agentIndex].y\n",
    "    \n",
    "    obs = [ax / (W-1), ay / (H-1)]\n",
    "    \n",
    "    if self.ghost_visible:\n",
    "        gx, gy = self.ghostCoords\n",
    "        obs += [gx / (W-1), gy / (H-1), 1.0]\n",
    "        dist_to_ghost = cheb_dist((ax, ay), (gx, gy)) / max(W-1, H-1)\n",
    "    else:\n",
    "        obs += [0.0, 0.0, 0.0]\n",
    "        dist_to_ghost = 0.0\n",
    "    \n",
    "    visible_agents = []\n",
    "    for other_idx in range(len(self.agents)):\n",
    "        if other_idx == agentIndex:\n",
    "            continue\n",
    "        if self.canSeeAgent(agentIndex, other_idx):\n",
    "            ox, oy = self.agents[other_idx].x, self.agents[other_idx].y\n",
    "            dist = cheb_dist((ax, ay), (ox, oy))\n",
    "            visible_agents.append((dist, ox, oy))\n",
    "    \n",
    "    visible_agents.sort(key=lambda x: x[0]) #Sort agent visibility by closest first\n",
    "    \n",
    "    for i in range(len(self.agents) - 1):\n",
    "        if i < len(visible_agents):\n",
    "            dist, ox, oy = visible_agents[i]\n",
    "            obs += [ox / (W-1), oy / (H-1), 1.0]\n",
    "        else:\n",
    "            obs += [0.0, 0.0, 0.0]\n",
    "    \n",
    "    obs += [\n",
    "        self.grid.extraction_area_tl[0]/(W-1), \n",
    "        self.grid.extraction_area_tl[1]/(H-1),\n",
    "        self.grid.extraction_area_br[0]/(W-1), \n",
    "        self.grid.extraction_area_br[1]/(H-1),\n",
    "        dist_to_ghost\n",
    "    ]\n",
    "\n",
    "    return obs\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#New State\n",
    "\n",
    "\n",
    "def get_state(self):\n",
    "    W, H = self.gridwidth, self.gridheight\n",
    "    s = []\n",
    "\n",
    "    # all agents\n",
    "    for a in self.engine.agents:\n",
    "        s += [a.x/(W-1), a.y/(H-1)]\n",
    "\n",
    "    if self.engine.ghost_visible:\n",
    "        gx, gy = self.engine.ghostCoords\n",
    "        s += [gx/(W-1), gy/(H-1), 1.0]\n",
    "    else:\n",
    "        s += [0.0, 0.0, 0.0]\n",
    "\n",
    "    #Extraction area\n",
    "    etlx, etly = self.engine.grid.extraction_area_tl\n",
    "    ebrx, ebry = self.engine.grid.extraction_area_br\n",
    "    s += [etlx/(W-1), etly/(H-1), ebrx/(W-1), ebry/(H-1)]\n",
    "\n",
    "    #Game progress\n",
    "    s += [self.engine.Time / self.episode_limit]\n",
    "\n",
    "    #Global task progress (for value estimation)\n",
    "    s += [\n",
    "        self.engine.surroundCounter / self.n_agents,\n",
    "        self.engine.holdCounter / self.engine.timeToKill,\n",
    "    ]\n",
    "    \n",
    "    #Only include avg distance to ghost if visible\n",
    "    if self.engine.ghost_visible:\n",
    "        avg_dist = sum(cheb_dist((a.x, a.y), self.engine.ghostCoords) for a in self.engine.agents) / (self.n_agents * max(W-1, H-1))\n",
    "        s += [avg_dist]\n",
    "    else:\n",
    "        s += [0.0]\n",
    "\n",
    "    return np.asarray(s, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2748b083",
   "metadata": {},
   "source": [
    "First attempt somme success towards end of training. I notice they spam in out of spotting it, so I will add decay to this reward. \n",
    "Given towards end of training, I will increase training time, and update epsilon for more exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa6ebaf",
   "metadata": {},
   "source": [
    "Adding reward for spread coverage when agent is not visible\n",
    "\n",
    "Curriculum Based: we start with large vision radius and ghost less mobile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dafaf562",
   "metadata": {},
   "source": [
    "<h2> Stage 1 </h2>\n",
    "\n",
    "  vision_radius: 10\n",
    "  spawn_radius: 11\n",
    "  ghost_move_prob: 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4cdb661",
   "metadata": {},
   "source": [
    "Statistically going pretty well. From video performance we see that once they see the ghost, they catch it very quickly, but they don't seem to be \"looking\" for it and rather bump into it pretty much at random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "96a87d83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"ResultImages/PartObvStage1_First.png\" width=\"900\" height=\"600\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url=\"ResultImages/PartObvStage1_First.png\", width=900, height=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2fd9d12",
   "metadata": {},
   "source": [
    "We will shape rewards for exploration, and reduce vision radius. This will force them into exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e66f6e3c",
   "metadata": {},
   "source": [
    "Video Name: FullObv_Stage1_FirstSuccess"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c537f3d7",
   "metadata": {},
   "source": [
    "Catching is working great, but it seems like they aren't doing much searching/exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41140105",
   "metadata": {},
   "source": [
    "<h2> Stage 2: Smaller vision, more mobile ghost </h2>\n",
    "\n",
    "We see that we can catch well, now let's incentivise searching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d4a8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GridCoverageReward(self):\n",
    "    exploration_reward = 0.0\n",
    "    \n",
    "    for agent in self.agents:\n",
    "        cell = (agent.x, agent.y)\n",
    "        visit_count = self.visit_map.get(cell, 0)\n",
    "        \n",
    "        # Reward inversely proportional to visit count\n",
    "        cell_reward = 1.0 / (1.0 + visit_count)\n",
    "        exploration_reward += self.lambda_grid_coverage * cell_reward\n",
    "        \n",
    "        # Update visit count\n",
    "        self.visit_map[cell] = visit_count + 1\n",
    "    \n",
    "    return exploration_reward\n",
    "\n",
    "\n",
    "\n",
    "##Global Reward Function \n",
    "\n",
    "...\n",
    "\n",
    "    ####Finding Ghost\n",
    "    if not prev_ghost_visible and self.ghost_visible:\n",
    "        reward += self.reward_ghost_spotted\n",
    "        self.reward_ghost_spotted *= 0.8  # Decay for future sightings\n",
    "\n",
    "    if self.ghost_visible:\n",
    "        reward += self.reward_ghost_visible\n",
    "\n",
    "    if not self.ghost_visible:\n",
    "        reward += self.GridCoverageReward()\n",
    "        reward += self.AgentSpreadReward()\n",
    "\n",
    "    ####Catching Ghost\n",
    "    if self.ghost_visible:\n",
    "        reward += self.AgentToGhostDist_Reward(prev_d_ghost_agents)\n",
    "        reward += self.Surround_Reward(prev_surround_count)\n",
    "        reward += self.QuadrantCoverage_Reward()  \n",
    "\n",
    "    return reward, terminated, success"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a947535",
   "metadata": {},
   "source": [
    "<h3> New from scratch policy </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "932c8bff",
   "metadata": {},
   "source": [
    "Video: PartObvStage2_Quite well\n",
    "Catch very well. Some issues with exploration, but not terrible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a4b383cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"ResultImages/PartObvStage2_FirstAttempt.png\" width=\"900\" height=\"600\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url=\"ResultImages/PartObvStage2_FirstAttempt.png\", width=900, height=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd57f3bd",
   "metadata": {},
   "source": [
    "Adding more incentive for exploration: only rewarding whewn they move. This should help the spazzy behaviour"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c6a850",
   "metadata": {},
   "source": [
    "Amazing results, statistically and video wise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b971a9",
   "metadata": {},
   "source": [
    "Video: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f86640b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"ResultImages/PartObv_Stage2_Winner.png\" width=\"900\" height=\"600\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url=\"ResultImages/PartObv_Stage2_Winner.png\", width=900, height=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d1115a7",
   "metadata": {},
   "source": [
    "NEXT STEPS: Even more difficult. Decrease vision radius, make ghost avoid earlier\n",
    "This policy was trained on top of stage 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4d254314",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"ResultImages/PartObv_Stage3.png\" width=\"900\" height=\"600\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url=\"ResultImages/PartObv_Stage3.png\", width=900, height=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbbb543b",
   "metadata": {},
   "source": [
    "TODO Video"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c89a87c2",
   "metadata": {},
   "source": [
    "We've clearly taught these agents to look for the ghost, and then to catch it once they do. Despite all the training, this is still a difficult task."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
